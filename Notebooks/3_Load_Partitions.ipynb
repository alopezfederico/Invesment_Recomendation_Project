{"cells":[{"cell_type":"markdown","source":["## Check and load partitions\n","\n","This code is a Python function designed to check and load data partitions from a local folder based on a DataFrame containing folder names and associated file lists. The function returns a dictionary with loaded DataFrames, each corresponding to a specific folder.\n","\n","The function returns a dictionary (dataframe_dicc) containing loaded DataFrames, where each key is the name of the corresponding folder.\n","\n"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"801c4581-61b0-4f8c-8c1e-ec00d8ab8b88"},{"cell_type":"markdown","source":["## Google Drive API: Pip install assistance"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"aa14940d-3bfe-466f-a500-2005469c89bc"},{"cell_type":"code","source":["pip install gdown google-api-python-client google-auth google-auth-oauthlib google-auth-httplib2"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"75228702-638b-401d-af11-858e354c4c4d","statement_id":5,"state":"finished","livy_statement_state":"available","queued_time":"2024-02-21T20:28:07.6036252Z","session_start_time":null,"execution_start_time":"2024-02-21T20:28:08.0412238Z","execution_finish_time":"2024-02-21T20:28:22.3139019Z","parent_msg_id":"c9fc0df4-eca8-4ee1-ade7-c237a7431543"},"text/plain":"StatementMeta(, 75228702-638b-401d-af11-858e354c4c4d, 5, Finished, Available)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Collecting gdown\n  Downloading gdown-5.1.0-py3-none-any.whl (17 kB)\nCollecting google-api-python-client\n  Downloading google_api_python_client-2.118.0-py2.py3-none-any.whl (12.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m150.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: google-auth in /home/trusted-service-user/cluster-env/trident_env/lib/python3.10/site-packages (2.23.3)\nRequirement already satisfied: google-auth-oauthlib in /home/trusted-service-user/cluster-env/trident_env/lib/python3.10/site-packages (1.0.0)\nCollecting google-auth-httplib2\n  Downloading google_auth_httplib2-0.2.0-py2.py3-none-any.whl (9.3 kB)\nRequirement already satisfied: beautifulsoup4 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.10/site-packages (from gdown) (4.12.2)\nRequirement already satisfied: filelock in /home/trusted-service-user/cluster-env/trident_env/lib/python3.10/site-packages (from gdown) (3.11.0)\nRequirement already satisfied: requests[socks] in /home/trusted-service-user/cluster-env/trident_env/lib/python3.10/site-packages (from gdown) (2.31.0)\nRequirement already satisfied: tqdm in /home/trusted-service-user/cluster-env/trident_env/lib/python3.10/site-packages (from gdown) (4.66.1)\nCollecting httplib2<1.dev0,>=0.15.0 (from google-api-python-client)\n  Downloading httplib2-0.22.0-py3-none-any.whl (96 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.9/96.9 kB\u001b[0m \u001b[31m38.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5 (from google-api-python-client)\n  Downloading google_api_core-2.17.1-py3-none-any.whl (137 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.0/137.0 kB\u001b[0m \u001b[31m55.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting uritemplate<5,>=3.0.1 (from google-api-python-client)\n  Downloading uritemplate-4.1.1-py2.py3-none-any.whl (10 kB)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.10/site-packages (from google-auth) (5.3.2)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.10/site-packages (from google-auth) (0.3.0)\nRequirement already satisfied: rsa<5,>=3.1.4 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.10/site-packages (from google-auth) (4.9)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.10/site-packages (from google-auth-oauthlib) (1.3.1)\nCollecting googleapis-common-protos<2.0.dev0,>=1.56.2 (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client)\n  Downloading googleapis_common_protos-1.62.0-py2.py3-none-any.whl (228 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m228.7/228.7 kB\u001b[0m \u001b[31m83.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (4.21.12)\nRequirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.10/site-packages (from httplib2<1.dev0,>=0.15.0->google-api-python-client) (3.0.9)\nRequirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth) (0.5.0)\nRequirement already satisfied: oauthlib>=3.0.0 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib) (3.2.2)\nRequirement already satisfied: soupsieve>1.2 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.10/site-packages (from beautifulsoup4->gdown) (2.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.10/site-packages (from requests[socks]->gdown) (3.3.1)\nRequirement already satisfied: idna<4,>=2.5 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.10/site-packages (from requests[socks]->gdown) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.10/site-packages (from requests[socks]->gdown) (1.26.17)\nRequirement already satisfied: certifi>=2017.4.17 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.10/site-packages (from requests[socks]->gdown) (2023.7.22)\nRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.10/site-packages (from requests[socks]->gdown) (1.7.1)\nInstalling collected packages: uritemplate, httplib2, googleapis-common-protos, google-auth-httplib2, google-api-core, gdown, google-api-python-client\nSuccessfully installed gdown-5.1.0 google-api-core-2.17.1 google-api-python-client-2.118.0 google-auth-httplib2-0.2.0 googleapis-common-protos-1.62.0 httplib2-0.22.0 uritemplate-4.1.1\nNote: you may need to restart the kernel to use updated packages.\n"]}],"execution_count":3,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":true},"nteract":{"transient":{"deleting":false}}},"id":"60a23a92-28eb-4ea6-9186-344e025a00f4"},{"cell_type":"markdown","source":["## Libraries Import"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"d4f109fd-a1fa-4381-a139-43bb7f000035"},{"cell_type":"code","source":["import os\n","import io\n","import pandas as pd\n","import builtin.utils as ut"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"75228702-638b-401d-af11-858e354c4c4d","statement_id":6,"state":"finished","livy_statement_state":"available","queued_time":"2024-02-21T20:28:07.6791037Z","session_start_time":null,"execution_start_time":"2024-02-21T20:28:22.7392629Z","execution_finish_time":"2024-02-21T20:28:26.2160415Z","parent_msg_id":"f4a9e1d1-8b71-4f51-9007-068b247557e8"},"text/plain":"StatementMeta(, 75228702-638b-401d-af11-858e354c4c4d, 6, Finished, Available)"},"metadata":{}}],"execution_count":4,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"b56b5d82-55bd-4273-af28-2eda1a66e9c4"},{"cell_type":"markdown","source":["## Check and Load Function\n","\n"," `check_and_load_partitions(local_folder_path, check_df_path)`\n","\n","Checks and loads partitions from the specified local folder based on a provided DataFrame with folder names and associated file lists, and returns a dictionary of loaded DataFrames.\n","\n","#### Parameters:\n","- `local_folder_path` (str): Local path of the folder to analyze.\n","- `check_df_path` (str): Path to the DataFrame file containing folder names and file lists.\n","\n","#### Returns:\n","- `dataframe_dicc` (dict): Dictionary containing loaded DataFrames with corresponding folder names.\n","\n","#### Example:\n","```python\n","check_and_load_partitions('/local/path/to/folder', '/path/to/check_dataframe.parquet')\n"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"52d514e7-682b-4f3d-bc94-148bae2a4e9a"},{"cell_type":"code","source":["# Check and load partitions based on a DataFrame with folder names and file lists\n","def check_and_load_partitions(local_folder_path, check_df_path):\n","    \"\"\"\n","    Checks and loads partitions from the specified local folder based on a provided DataFrame\n","    with folder names and associated file lists, and returns a dictionary of loaded DataFrames.\n","\n","    Parameters:\n","    - local_folder_path (str): Local path of the folder to analyze.\n","    - check_df_path (str): Path to the DataFrame file containing folder names and file lists.\n","\n","    Returns:\n","    - dataframe_dicc (dict): Dictionary containing loaded DataFrames with corresponding folder names.\n","\n","    Example:\n","    - check_and_load_partitions('/local/path/to/folder', '/path/to/check_dataframe.parquet')\n","    \"\"\"\n","\n","    # Initialize an empty dictionary to store loaded DataFrames\n","    dataframe_dicc = {}\n","    dataframe_object = pd.DataFrame()\n","\n","    # Read the check DataFrame from the specified path\n","    check_df = pd.read_parquet(check_df_path)\n","\n","    # Iterate through each row in the check DataFrame\n","    for row in check_df.itertuples():\n","        check_name = row.name\n","        check_list = row.files\n","        aux_folder_path = os.path.join(local_folder_path, check_name)\n","\n","        # Check if the folder corresponding to the current row exists locally\n","        if os.path.isdir(aux_folder_path):\n","            aux_list = os.listdir(aux_folder_path)\n","            dataframe_list = []\n","\n","            # Iterate through each file in the folder\n","            for check_file in aux_list:\n","\n","                # Check if the file is not in the expected file list\n","                if not check_file in check_list:\n","                    potential_file_path = os.path.join(aux_folder_path, check_file)\n","                    \n","                    # Read the JSON file into a DataFrame\n","                    aux_df = pd.read_json(potential_file_path, lines=True)\n","                    dataframe_list.append(aux_df)\n","                \n","                else:\n","                    potential_file_path = pd.DataFrame()\n","                    # Read the JSON file into a DataFrame\n","                    aux_df = potential_file_path\n","                    dataframe_list.append(aux_df)\n","\n","            # Concatenate DataFrames from the folder and add a 'date' column\n","           \n","            dataframe_object = pd.concat(dataframe_list, axis=0, ignore_index=True)\n","            if not dataframe_object.empty:\n","                dataframe_object['date'] = dataframe_object['time'].apply(ut.mili_to_datetime)\n","\n","            # Store the DataFrame in the dictionary with the folder name as the key\n","            dataframe_dicc[check_name] = dataframe_object\n","            print(f'{check_name}: Partition Loaded 100%')\n","\n","        # Check if the row corresponds to the main folder\n","        elif check_name == local_folder_path.split('/')[-1]:\n","            aux_list = os.listdir(local_folder_path)\n","            dataframe_list = []\n","\n","            # Iterate through each file in the main folder\n","            for check_file in aux_list:\n","                # Check if the file is not in the expected file list\n","                if not check_file in check_list:\n","                    potential_file_path = os.path.join(local_folder_path, check_file)\n","                    \n","                    # Read the JSON file into a DataFrame\n","                    aux_df = pd.read_json(potential_file_path, lines=True)\n","                    dataframe_list.append(aux_df)\n","                \n","                else:\n","                    potential_file_path = pd.DataFrame()\n","                    # Read the JSON file into a DataFrame\n","                    aux_df = potential_file_path\n","                    dataframe_list.append(aux_df)\n","\n","\n","            # Concatenate DataFrames from the main folder and add a 'date' column\n","            \n","            dataframe_object = pd.concat(dataframe_list, axis=0, ignore_index=True)\n","            \n","            \n","            # Store the DataFrame in the dictionary with the folder name as the key\n","            dataframe_dicc[check_name] = dataframe_object\n","            print(f'{check_name}: Partition Loaded 100%')\n","\n","    return dataframe_dicc\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"75228702-638b-401d-af11-858e354c4c4d","statement_id":7,"state":"finished","livy_statement_state":"available","queued_time":"2024-02-21T20:28:07.7407993Z","session_start_time":null,"execution_start_time":"2024-02-21T20:28:26.6504418Z","execution_finish_time":"2024-02-21T20:28:26.8754856Z","parent_msg_id":"6292295e-eada-4726-bddf-1e735c160dab"},"text/plain":"StatementMeta(, 75228702-638b-401d-af11-858e354c4c4d, 7, Finished, Available)"},"metadata":{}}],"execution_count":5,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"229eeee0-0071-4ef6-8fc2-7dc7f3151a15"},{"cell_type":"markdown","source":["## Update Database Function\n","\n"," `update_database(dataframe_dicc, database_path)`\n","\n","Updates a database with DataFrames from a dictionary and returns the updated dictionary.\n","\n","#### Parameters:\n","- `dataframe_dicc` (dict): Dictionary containing DataFrames to be added to the database.\n","- `database_path` (str): Path to the database folder.\n","\n","#### Returns:\n","- `dicc` (dict): Updated dictionary containing DataFrames.\n","\n","#### Example:\n","```python\n","update_database({'review-parquet1': df1, 'review-parquet2': df2}, '/path/to/database')\n"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"09ab3292-5249-4acd-a8de-5558f0cf0a31"},{"cell_type":"code","source":["# Update a database with DataFrames from a dictionary\n","def update_database(dataframe_dicc, database_path):\n","    \"\"\"\n","    Updates a database with DataFrames from a dictionary and returns the updated dictionary.\n","\n","    Parameters:\n","    - dataframe_dicc (dict): Dictionary containing DataFrames to be added to the database.\n","    - database_path (str): Path to the database folder.\n","\n","    Returns:\n","    - dicc (dict): Updated dictionary containing DataFrames.\n","\n","    Example:\n","    - update_database({'review-parquet1': df1, 'review-parquet2': df2}, '/path/to/database')\n","    \"\"\"\n","\n","    # Initialize an empty dictionary to store DataFrames\n","    dicc = {}\n","\n","    # Get the name of the first file in the database path\n","    first_file = os.listdir(database_path)[0].split('.')[0]\n","\n","    # Check if the first file matches the key of the provided dictionary\n","    if first_file == list(dataframe_dicc.keys())[0]:\n","        file_path = os.path.join(database_path, os.listdir(database_path)[0])\n","        parquet_name = first_file\n","\n","        # Read the existing DataFrame from the first file in the database\n","        db_df = pd.read_parquet(file_path)\n","\n","        # Extract and concatenate the DataFrame from the provided dictionary\n","        if not len(dataframe_dicc[first_file]) == 0:\n","            dicc_df = pd.DataFrame(dataframe_dicc[first_file])\n","        else:\n","            dicc_df = pd.DataFrame()\n","            \n","\n","        dataframe_object = pd.concat([db_df, dicc_df], axis=0, ignore_index=True)\n","\n","        # Add the concatenated DataFrame to the updated dictionary\n","        dicc[parquet_name] = dataframe_object\n","\n","        return dicc\n","\n","    else:\n","        # Iterate through each file in the database path\n","        for parquet in os.listdir(database_path):\n","\n","            file_path = os.path.join(database_path, parquet)\n","\n","            # Construct the name for the key in the updated dictionary\n","            parquet_name = f\"review-{parquet.split('.')[0]}\"\n","\n","            # Read the existing DataFrame from the current file in the database\n","            db_df = pd.read_parquet(file_path)\n","\n","            # Extract and concatenate the DataFrame from the provided dictionary\n","            if not len(dataframe_dicc[parquet_name]) == 0:\n","                dicc_df = pd.DataFrame(dataframe_dicc[parquet_name])\n","            else:\n","                dicc_df = pd.DataFrame()\n","                \n","            dataframe_object = pd.concat([db_df, dicc_df], axis=0, ignore_index=True)\n","\n","            # Add the concatenated DataFrame to the updated dictionary with the appropriate key\n","            aux_name = parquet.split('.')[0]\n","            dicc[aux_name] = dataframe_object \n","\n","    return dicc\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"75228702-638b-401d-af11-858e354c4c4d","statement_id":8,"state":"finished","livy_statement_state":"available","queued_time":"2024-02-21T20:28:07.7994822Z","session_start_time":null,"execution_start_time":"2024-02-21T20:28:27.2743797Z","execution_finish_time":"2024-02-21T20:28:27.5172294Z","parent_msg_id":"657f36fd-758e-43bd-908b-a39e4d385cba"},"text/plain":"StatementMeta(, 75228702-638b-401d-af11-858e354c4c4d, 8, Finished, Available)"},"metadata":{}}],"execution_count":6,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"440cae1e-ffe9-4986-bbcb-1316d7e07ff6"},{"cell_type":"markdown","source":["## Exercise Test"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"bb2c275c-7978-4bfe-808c-8ab548490fc4"},{"cell_type":"code","source":["sitios_path = '/lakehouse/default/Files/otra_prueba/metadataSitiosPrueba'\n","sitios_check = '/lakehouse/default/Files/otra_prueba/pruebaList/metadataSitiosPrueba.parquet'\n","sitios_dicc = check_and_load_partitions(sitios_path, sitios_check)\n","prueba_path = '/lakehouse/default/Files/otra_prueba/databasePrueba'\n","\n","#Actualizar la data\n","database_sitios = '/lakehouse/default/Files/otra_prueba/databasePrueba/Metadata_sitios_parquet'\n","sitios_dicc= update_database(sitios_dicc, database_sitios)\n","#Montar la data\n","ut.dataframe_to_parquet(sitios_dicc, 'Metadata_sitios_parquet',prueba_path)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"75228702-638b-401d-af11-858e354c4c4d","statement_id":9,"state":"finished","livy_statement_state":"available","queued_time":"2024-02-21T20:28:07.8885862Z","session_start_time":null,"execution_start_time":"2024-02-21T20:28:27.915514Z","execution_finish_time":"2024-02-21T20:28:50.8331619Z","parent_msg_id":"97dfa624-fde5-47c8-8c32-228559863cea"},"text/plain":"StatementMeta(, 75228702-638b-401d-af11-858e354c4c4d, 9, Finished, Available)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["metadataSitiosPrueba: Partition Loaded 100%\nDataframes saved successfully in df_database/Metadata_sitios_parquet\n"]}],"execution_count":7,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"3bf763cd-004a-496b-8223-b20adb1e4a61"},{"cell_type":"code","source":["estados_path = '/lakehouse/default/Files/otra_prueba/reviewEstadosPrueba'\n","estados_check = '/lakehouse/default/Files/otra_prueba/pruebaList/reviewEstadosPrueba.parquet'\n","estados_dicc = check_and_load_partitions(estados_path, estados_check)\n","prueba_path = '/lakehouse/default/Files/otra_prueba/databasePrueba'\n","\n","#Actualizar la data\n","database_estados = '/lakehouse/default/Files/otra_prueba/databasePrueba/Review_estados_parquet'\n","estados_dicc= update_database(estados_dicc, database_estados)\n","#Montar la data\n","ut.dataframe_to_parquet(estados_dicc, 'Review_estados_parquet',prueba_path)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"75228702-638b-401d-af11-858e354c4c4d","statement_id":10,"state":"finished","livy_statement_state":"available","queued_time":"2024-02-21T20:28:07.9589912Z","session_start_time":null,"execution_start_time":"2024-02-21T20:28:51.2158414Z","execution_finish_time":"2024-02-21T20:28:56.095549Z","parent_msg_id":"a4a946b0-c913-4b5f-ac96-795a421db077"},"text/plain":"StatementMeta(, 75228702-638b-401d-af11-858e354c4c4d, 10, Finished, Available)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["review-Texas: Partition Loaded 100%\nreview-Washington: Partition Loaded 100%\nreview-Wyoming: Partition Loaded 100%\nDataframes saved successfully in df_database/Review_estados_parquet\nDataframes saved successfully in df_database/Review_estados_parquet\n"]}],"execution_count":8,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"393811a7-d84b-4cda-9b31-8e8758048ce7"},{"cell_type":"markdown","source":["## Original Routing:\n","\n","Since this point. All routes used are related with the original database. Here we can find all path needed in order to link information extracted from the requested drive folder and database created"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"f7e414bf-e2fe-4fa6-b190-47a908be3aa5"},{"cell_type":"markdown","source":["## Metadata - Sitios Partition Update"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"1ada6e5d-1e09-46ca-869e-54bbb63aa993"},{"cell_type":"code","source":["'''estado_path = '/lakehouse/default/Files/original/reviews-estados'\n","estado_listCheck = '/lakehouse/default/Files/notes_and_list/reviews-estados.parquet'\n","\n","estadoDicc = check_and_load_partitions(estado_path, estado_listCheck)\n","df_database = '/lakehouse/default/Files/df_database'\n","\n","#Data Update\n","estado_database = '/lakehouse/default/Files/df_database/Review_estados_parquet'\n","estadoDicc = update_database(estadoDicc, estado_database)\n","\n","#Upload Data\n","ut.dataframe_to_parquet(estadoDicc,'Review_estados_parquet',df_database)'''"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"75228702-638b-401d-af11-858e354c4c4d","statement_id":11,"state":"finished","livy_statement_state":"available","queued_time":"2024-02-21T20:28:08.0235051Z","session_start_time":null,"execution_start_time":"2024-02-21T20:28:56.5446664Z","execution_finish_time":"2024-02-21T20:28:56.7872236Z","parent_msg_id":"c17dd879-214c-4341-a63d-de39f2813012"},"text/plain":"StatementMeta(, 75228702-638b-401d-af11-858e354c4c4d, 11, Finished, Available)"},"metadata":{}},{"output_type":"execute_result","execution_count":32,"data":{"text/plain":"\"estado_path = '/lakehouse/default/Files/original/reviews-estados'\\nestado_listCheck = '/lakehouse/default/Files/notes_and_list/reviews-estados.parquet'\\n\\nestadoDicc = check_and_load_partitions(estado_path, estado_listCheck)\\ndf_database = '/lakehouse/default/Files/df_database'\\n\\n#Data Update\\nestado_database = '/lakehouse/default/Files/df_database/Review_estados_parquet'\\nestadoDicc = update_database(estadoDicc, estado_database)\\n\\n#Upload Data\\nut.dataframe_to_parquet(estadoDicc,'Review_estados_parquet',df_database)\""},"metadata":{}}],"execution_count":9,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"5384b4e7-c915-4fe7-9724-7b0675af5d78"},{"cell_type":"markdown","source":["## Review - Estados Partition Update"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"e57d344a-4fc4-4d53-a338-f32360533d8d"},{"cell_type":"code","source":["'''metadata_path = '//lakehouse/default/Files/original/metadata-sitios'\n","metadata_listCheck = '/lakehouse/default/Files/notes_and_list/metadata-sitios.parquet'\n","\n","#metadataDicc = check_and_load_partitions(metadata_path, metadata_listCheck)\n","#df_database = '/lakehouse/default/Files/df_database'\n","\n","#Data Update\n","metadata_database = '/lakehouse/default/Files/df_database/Metadata_sitios_parquet'\n","metadataDicc = update_database(metadataDicc, metadata_database)\n","\n","#Upload Data\n","ut.dataframe_to_parquet(metadataDicc,'Metadata_sitios_parquet',df_database)'''"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"75228702-638b-401d-af11-858e354c4c4d","statement_id":12,"state":"finished","livy_statement_state":"available","queued_time":"2024-02-21T20:28:08.0793952Z","session_start_time":null,"execution_start_time":"2024-02-21T20:28:57.2853773Z","execution_finish_time":"2024-02-21T20:28:57.5147862Z","parent_msg_id":"67f6bf87-3750-438a-8adb-07ed848cec83"},"text/plain":"StatementMeta(, 75228702-638b-401d-af11-858e354c4c4d, 12, Finished, Available)"},"metadata":{}},{"output_type":"execute_result","execution_count":35,"data":{"text/plain":"\"metadata_path = '//lakehouse/default/Files/original/metadata-sitios'\\nmetadata_listCheck = '/lakehouse/default/Files/notes_and_list/metadata-sitios.parquet'\\n\\n#metadataDicc = check_and_load_partitions(metadata_path, metadata_listCheck)\\n#df_database = '/lakehouse/default/Files/df_database'\\n\\n#Data Update\\nmetadata_database = '/lakehouse/default/Files/df_database/Metadata_sitios_parquet'\\nmetadataDicc = update_database(metadataDicc, metadata_database)\\n\\n#Upload Data\\nut.dataframe_to_parquet(metadataDicc,'Metadata_sitios_parquet',df_database)\""},"metadata":{}}],"execution_count":10,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"5ed4141c-a0cf-49e0-9fae-4d0c8dbe2f2e"}],"metadata":{"language_info":{"name":"python"},"microsoft":{"language":"python"},"widgets":{},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"kernel_info":{"name":"synapse_pyspark"},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default"},"trident":{"lakehouse":{"default_lakehouse":"12da7d29-4cfa-4cf2-b63c-6f5efdefceac","default_lakehouse_name":"Data","default_lakehouse_workspace_id":"16553314-54c2-4772-8a43-dad6b1ad9755"}}},"nbformat":4,"nbformat_minor":5}